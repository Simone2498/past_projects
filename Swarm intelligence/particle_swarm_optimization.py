# -*- coding: utf-8 -*-
"""Particle Swarm Optimization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bV6pXguQvyIvouCnbIkzo5bG8Lb7bt-1
"""

# Imports
import math
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import pandas as pd
import numpy as np
from random import random
import seaborn as sns

# Constants
B = 1

AXIS_X_LFT = -3
AXIS_X_RGT = 3
AXIS_Y_BOT = -5
AXIS_Y_TOP = 5

INF=float('inf')
_INF=float('-inf')

N_PARTICLES = 7

"""#Benchmark functions#
## Rosenbrock ##
$$ f(x,y)=(a-x)^{2}+b(y-x^{2})^{2} $$
## Rastrigin ##
$$ f(X)=10n + \sum_{i+1}^{n}(x_{i}^{2}-10\cos(2\pi x_{i})) $$

"""

#David code
""" Value functions """

def Rosenbrock(X, b = B):
    x=X[0]
    y=X[1]
    a = 0
    f = pow((a-pow(x, 2)), 2) + b * pow((y - pow(x, 2)), 2)
    return f

def Rastrigin(X):
    n = len(X)
    sum = 0
    for x in X:
        sum += pow(x, 2) - 10 * math.cos(2 * math.pi * x)
    f = 10 * n + sum
    return f

"""#PSO implementation#
Here there is all the code for the PSO, Particle represent the single element/robot, search_min is a wrapper that simulates environment and time.
Hyperparameters apart, to use this function you need to pass the banchmark function to search_min as performance, set the bounds as an array of the form [[dim1_min, dim1_max],[dim2_min, ...]] and define the precision that fits your problem.

Hyperparameters:
1. P = population
1. a = inertial coef
1. a_decay = inertial weight decay rate
1. b = my best position weight
1. c = neightbours best position weight
1. Vmax = max speed reachable
1. n_dist = range of comunication, distance to consider a particle a neightbour


"""

#Simone Code
""" 
The class Particle provides the basic functions for the elements, I provide all the hyperparameters by a coordinator/wrapper function
"""
class Particle(): 
    def __init__(self, a, a_decay, b, c, n_dist, Vmax, bounds, performance):
        #Hyperparameters
        self.a=a
        self.a_decay=a_decay #a decay rate
        self.b=b 
        self.c=c 
        self.Vmax=Vmax
        self.n_dist=n_dist #Distance to consider a particle "neightbour", in a real environment there could be other (and more efficient) strategies such as the portance of the sensors
        #Informations about the performance function
        self.bounds=np.array(bounds) #I can set the function bounds, otherwise I can use the machine representation limits
        self.N=len(self.bounds) #number of dimension
        self.performance=performance #Performance function
        #Data about the particels' state
        self.s=np.multiply(np.random.rand(self.N), (self.bounds[:,1]-self.bounds[:,0]))+self.bounds[:,0] #random initialized position
        self.v=np.zeros(self.N) #No initial speed
        self.f=self.performance(self.s)
        self.fbest=INF #initialized to inf to find a minimum or max to find a max
        self.pbest=self.s #initial best position equal to the starting one
    #Particle state update
    def update(self):
        R=random() #random number to balance exploration and exploitation
        gbest=self.get_gbest() #get the best neightbour's position
        self.v=self.a*self.v+self.b*R*(self.pbest-self.s)+self.c*R*(gbest-self.s) #calculate the new speed
        #check the speed bounds
        #self.v=max(self.v, 0)
        self.v=np.minimum(self.v, self.Vmax)
        dt=1 #I assume time step of 1
        self.s=self.s+self.v*dt #I calculate the new position
        #check the bounds of the position
        self.s=np.maximum(self.s,self.bounds[:,0])
        self.s=np.minimum(self.s, self.bounds[:,1])
        #evaluate the current position and update the pbest
        self.f=self.performance(self.s)
        if(self.f<self.fbest):
            self.fbest=self.f 
            self.pbest=self.s
        self.a=max(self.a-self.a_decay, 0.4) #I assume to fix the min value of a to 0.4, it could be another tunable hyperparameter

    def distance(self, s_my, s_oth): #euclidean distance
        return np.linalg.norm(s_my-s_oth)

    def get_gbest(self): #find among the neightbours the best one and return its position
        gbest=[]
        fmin=INF
        for p in range(len(particels)): #among all the particels
            if(self.distance(self.s, particels[p].s)<self.n_dist): #check the distance, in this scenario I assume to know the position of all the neightbours, in real environment could be used other strategies such as sensors or social neightbouring
                if(fmin>particels[p].f):
                    fmin=particels[p].f
                    gbest=particels[p].s
        return gbest

#Simone Code
particels=[]
positions=[[],[]]
def search_min(P, a, a_decay, b, c, Vmax, n_dist, min_err, performance, bounds, animation=False):
    #Wrapper for the environment, it simulates the time step. In a real environment Particels can update by themselves
    for i in range(P):
        particels.append(Particle(a, a_decay, b, c, n_dist, Vmax, bounds, performance))
    f=INF
    I=1000 #max number of iteration, only to make the hyparameters tuning feasible
    for i in range(I): #max iteration, I can use a early stopping for too little value changes
        err=0 #maximum variation obtained in the current iteration, it could be interpretated as the precision we have reach
        if animation:
          positions[0].append([])
          positions[1].append([])
        for p in range(P):
            particels[p].update()
            if animation:
              positions[0][i].append(particels[p].s[0])
              positions[1][i].append(particels[p].s[1])
            if f>particels[p].f:
                f=particels[p].f
                pos=particels[p].s
            if abs(f-particels[p].f)>err:
              err=abs(f-particels[p].f)
        if err<min_err: # early stopping
          break
    return f, pos

"""## Execution of thee search without animation ##"""

if __name__=="__main__":
    #@ performance = our function
    f, pos=search_min(N_PARTICLES, 1, 0.0005, 2, 2, 100, 100, 0.001, Rosenbrock, [[AXIS_X_LFT,AXIS_X_RGT],[AXIS_Y_BOT,AXIS_Y_TOP]])
    print(f, pos)

"""## Execution with animation ##"""

#David Code
Writer = animation.writers['ffmpeg']
writer = Writer(fps=8, metadata=dict(artist='Me'), bitrate=1800)
#Init of the function graph
fig = plt.figure(figsize=(10,6))
plt.xlim(AXIS_X_LFT, AXIS_X_RGT)
plt.ylim(AXIS_Y_BOT, AXIS_Y_TOP)
plt.xlabel('X')
plt.ylabel('Y')
plt.title('plot')
#Benchmark functions
a = 0; b = 1
x = np.arange(AXIS_X_LFT, AXIS_X_RGT, 0.1)
y = np.arange(AXIS_Y_BOT, AXIS_Y_TOP, 0.1)
xx, yy = np.meshgrid(x, y)

z = ((a-(xx**2))**2) + b * ((yy - (xx**2))**2) #Rosenbrock
#z = 10 * 2 + ((xx**2) - 10 * np.cos(2 * math.pi * xx)) + ((yy**2) - 10 * np.cos(2 * math.pi * yy)) #Rastrigin

df1 = pd.DataFrame(data=z, index=y, columns=x)
df1.style.background_gradient(cmap ='inferno').set_properties(**{'font-size': '20px'}) 
h = plt.contourf(x,y,z, levels=1000)

#Business logic
particels=[]
positions=[[],[]]
f,pos=search_min(N_PARTICLES, 1, 0.0033, 2, 2, 1.4, 100, 0.001, Rosenbrock, [[AXIS_X_LFT,AXIS_X_RGT],[AXIS_Y_BOT,AXIS_Y_TOP]], animation=True)
positions=np.array(positions)
print('Value: ', f, 'Position: ', pos)
ln, = plt.plot([], [], 'ro')
def update(f): #Frame's update 
    print(f)
    ln.set_data(positions[0][f], positions[1][f])
    return ln,

ani = matplotlib.animation.FuncAnimation(fig, update, frames=positions.shape[1], interval = 1, blit = False) #to print all the frames frames=positions.shape[1]
ani.save('testrun.mp4', writer=writer)
plt.show()

"""#Considration about GD#
1. In Rastigin, too many local minima to have a reliable global minimum detection, we can add momentum (Rumelhart at all) or balance exploration and exploitation with a GLIE method.
In Rosenbrock the valley brings to a gradient disapear, so GD easily reach the valley and then becomes very slow, a weight decay method could improve the performance.
1. All the updates are sequence dependent and need to be performed on the same device, it's not possible a parameterisation.
1. We could implement a local gradient descent in each particels, when we update we consider the current speed, my best position, my neightbours' best position and the result of a local GD search.
Another approach could be consider only the direction of the gradient, without a full local search.


1. Comparing the performance, PSO will more likely find the global optimum for Rastragin, compared to GD.

1. The combination of the travel distance with the weights for the inclusion of the best neighbor values determine the cohesion of the swarm, meaning that larger maximum velocity implies less cohesion, considering the random movements in other directions.

1. How about employing a minimal distance between each particle?

##Test of tuning with evolutionary algorithm##
"""

#Simone Code
def evolution():
    P_num=10
    Time=10
    population=[]
    fit=[]
    #I create the population
    for p in range(P_num):
        population.append([(int( random()*10 )+1), (random()*10), (random()*0.1), (random()*10), (random()*10), (random()*50), (random()*5)])
        fit.append(0)
    history=[]
    for t in range(Time):
        print('Generation: ', t)
        #Fit filter
        best_1=-1; val_best_1=INF
        best_2=-1; val_best_2=INF
        worst=-1; val_worst=0
        for p in range(P_num):
            print(' Fit: ', p)
            fit[p], pos = search_min(int(population[p][0]), population[p][1], population[p][2], population[p][3], population[p][4], population[p][5], population[p][6], 0.001, Rastrigin, [[AXIS_X_LFT,AXIS_X_RGT],[AXIS_Y_BOT,AXIS_Y_TOP]]) 
            if fit[p]<val_best_1:
                if val_best_1<val_best_2:
                    val_best_2=val_best_1
                    best_2=best_1
                val_best_1=fit[p]
                best_1=p
            elif fit[p]<val_best_1:
                val_best_2=fit[p]
                best_2=p
            elif fit[p]>val_worst:
                val_worst=fit[p]
                worst=p
        history.append(best_1)
        #Reproduction and mutation
        population[worst]=reproduction(population[best_1],population[best_2])
    plt.plot(history)
    print(population[best_1])

def reproduction(a, b):
    c=[]
    mutation=[int(random()*10)-10, random()*10, random()*0.1, random()*10, random()*10, random()*50, random()*5]
    bounds_low=[1, 0, 0, 0, 0, 0, 0]
    bounds_high=[10, 10, 0.1, 10, 10, 50, 5]
    for g in range(len(a)):
        c.append((a[g]+b[g])/2)
        #c[g]+=0.1*mutation[g] #Active for mutation
        if (g==0): c[g]=int(c[g])
        c[g]=max(c[g], bounds_low[g])
        c[g]=min(c[g], bounds_high[g])
    return c

evolution()